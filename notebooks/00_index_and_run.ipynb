{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run Moatless Tools",
   "id": "7fea6d23c616b470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, index the codebase in a vector store.\n",
    "\n",
    "Set `repo_dir` to the path of the repository you want to index."
   ],
   "id": "69c80c1cba7d5c37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T17:49:18.101579Z",
     "start_time": "2024-06-09T17:49:05.244903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moatless.index import CodeIndex, IndexSettings\n",
    "from moatless.benchmark.swebench import setup_swebench_repo, get_repo_dir_name\n",
    "from moatless import FileRepository, Settings, Workspace\n",
    "\n",
    "import os\n",
    "\n",
    "# An OPENAI_API_KEY is required to use the OpenAI Models\n",
    "Settings.agent_model = \"gpt-4o-2024-05-13\"\n",
    "index_settings = IndexSettings(\n",
    "    embed_model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "repo_dir = \"/tmp/moatless\"\n",
    "file_repo = FileRepository(repo_path=repo_dir)\n",
    "\n",
    "code_index = CodeIndex(file_repo=file_repo, settings=index_settings)\n",
    "nodes, tokens = code_index.run_ingestion()\n",
    "\n",
    "print(f\"Indexed {nodes} nodes and {tokens} tokens\")\n",
    "\n",
    "workspace = Workspace(file_repo=file_repo, code_index=code_index)"
   ],
   "id": "b9dd5259592cc65e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parsing nodes:   0%|          | 0/41 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe665cffe2124a379a246de04bd75133"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings:   0%|          | 0/177 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8bf2f45653441079c2e1d09298f5970"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 177 nodes and 58057 tokens\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then use the `SearchLoop` to find the relevant code.",
   "id": "bc13c37fd492267f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T17:49:20.378781Z",
     "start_time": "2024-06-09T17:49:18.103212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moatless.loop import SearchLoop\n",
    "\n",
    "instructions = \"Remove the token limit check from the completion funktion\"\n",
    "\n",
    "search_loop = SearchLoop(workspace, instructions=instructions)\n",
    "search_response = search_loop.execute()\n",
    "\n",
    "for file in search_response.files:\n",
    "    print(f\"File: {file.file_path}\")\n",
    "    print(f\"Spans:\")\n",
    "    for span_id in file.span_ids:\n",
    "        print(f\" - {span_id}\")"
   ],
   "id": "38c6f7f6422053fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: moatless/llm/completion.py\n",
      "Spans:\n",
      " - completion\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Execute the `CodeLoop` to apply the changes.",
   "id": "c160cc8c9c7e202e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T17:49:25.087652Z",
     "start_time": "2024-06-09T17:49:20.382258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moatless.loop import  CodeLoop\n",
    "\n",
    "coder = CodeLoop(workspace, instructions=instructions, files=search_response.files)\n",
    "code_response = coder.execute()\n",
    "\n",
    "print(f\"Response: {code_response.message}\")"
   ],
   "id": "903b67c9dff5c384",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The token limit check has been successfully removed from the completion function as requested.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run a `$ git diff` to see the changes.",
   "id": "5d131ca3793b26a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T17:49:25.101291Z",
     "start_time": "2024-06-09T17:49:25.091965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.run(\n",
    "      [\"git\", \"diff\"],\n",
    "      capture_output=True,\n",
    "      text=True,\n",
    "      cwd=repo_dir,\n",
    ")\n",
    "\n",
    "print(output.stdout)"
   ],
   "id": "d51ba9eceb0b7288",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/moatless/llm/completion.py b/moatless/llm/completion.py\n",
      "index 2c95e47..a926948 100644\n",
      "--- a/moatless/llm/completion.py\n",
      "+++ b/moatless/llm/completion.py\n",
      "@@ -48,10 +48,6 @@ def completion(\n",
      " \n",
      "     metadata[\"trace_name\"] = trace_name\n",
      " \n",
      "-    tokens = token_counter(messages=messages[-1:])\n",
      "-    if tokens > Settings.max_message_tokens:\n",
      "-        raise ValueError(f\"Too many tokens in the new message: {tokens}\")\n",
      "-\n",
      "     response = litellm.completion(\n",
      "         model=model,\n",
      "         max_tokens=max_tokens,\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
